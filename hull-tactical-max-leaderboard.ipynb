{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc3aae5",
   "metadata": {
    "papermill": {
     "duration": 0.002703,
     "end_time": "2025-11-04T11:58:48.848823",
     "exception": false,
     "start_time": "2025-11-04T11:58:48.846120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hull Tactical Market Prediction – Public LB Maximization\n",
    "\n",
    "> ⚠️ **Important Note:** The public leaderboard in this competition does **not** matter.  \n",
    "> All test data is already included in the training set, so leaderboard scores are purely illustrative.  \n",
    "> This work was done only to better understand the evaluation metric and how strategies interact with it.\n",
    "\n",
    "---\n",
    "## TLDR\n",
    "\n",
    "**Evaluation metric:** Adjusted Sharpe — maximize mean excess return, penalized only if  \n",
    "  - strategy volatility > 1.2× market, or  \n",
    "  - strategy underperforms the market.  \n",
    "  → Optimal strategies sit just below the 1.2× vol cap.  \n",
    "\n",
    "\n",
    "**What’s useful:**  \n",
    "  - **Vol targeting:** scale exposures so strategy volatility ≈ 1.199× market.  \n",
    "  - **Thresholding:** filter out tiny positives that add variance but little mean.  \n",
    "  - **Simple mapping:** use constant α or a small tiered scheme; tune with CV against the official metric.  \n",
    "\n",
    "\n",
    "**What’s not useful:**  \n",
    "  - Public LB “perfect foresight” scores — these exploit leakage and don’t matter for the actual competition.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Initial Approach\n",
    "The starting strategy was the “perfect foresight” method, inspired by Veniamin Nelin’s excellent notebook:\n",
    "\n",
    "- **Rule:** If the forward return for a date was positive, set exposure to the max allowed (2). Otherwise, set exposure to the min (0).  \n",
    "- **Effect:** Always fully invested on up days and completely out on down days.  \n",
    "- **Result:** Produced a strong adjusted Sharpe (~**10.147**) on the public leaderboard.\n",
    "\n",
    "---\n",
    "\n",
    "## Intermediate Exploration\n",
    "We next experimented with magnitude-aware scaling:\n",
    "\n",
    "- **Idea:** Scale exposure smoothly (linear/sqrt mappings) and ignore small positives.  \n",
    "- **Goal:** Reduce volatility and improve Sharpe by focusing on stronger positive-return days.  \n",
    "- **Outcome:** This reduced the mean return more than it reduced volatility, dropping the score to ~**9.77**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insight from the Metric\n",
    "Looking closely at the evaluation code revealed:\n",
    "\n",
    "- A **volatility penalty** only applies if strategy vol > 1.2× the market’s.  \n",
    "- A **return penalty** only applies if the strategy underperforms the market.  \n",
    "- Otherwise, the metric is just Sharpe — so the optimal path is to **maximize Sharpe while sitting just under the 1.2× cap**.\n",
    "\n",
    "---\n",
    "\n",
    "## Refined Approach\n",
    "The adjustment was to use the entire volatility budget:\n",
    "\n",
    "- **Binary tuning:** Instead of always using 2.0 on positive days, tune a constant **α** so that overall strategy volatility sits right at the 1.2× cap.  \n",
    "- **Two-level refinement:** Apply full 2.0 exposure to the top quantile of positive days, and α on the rest, again tuned to respect the volatility boundary.  \n",
    "- **Thresholding:** Add a small cutoff to trim micro-positives that added volatility but little mean return.\n",
    "\n",
    "This way, the strategy doesn’t leave volatility “unused” and directs more exposure to the highest-return days.\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "- **Original binary rule:** ~10.147  \n",
    "- **Magnitude scaling (failed):** ~9.77  \n",
    "- **Two-level refinement:** ~10.164  \n",
    "- **Threshold-tuned single-level:** **10.204**\n",
    "\n",
    "---\n",
    "\n",
    "## Takeaways\n",
    "- The initial “all-in on positive days, out on negative days” approach is already highly effective under the competition’s rules.  \n",
    "- Magnitude scaling without regard to the penalty structure reduced performance.  \n",
    "- Targeting the **volatility cap** directly and allocating exposure efficiently across positive days provides measurable lift.  \n",
    "- With careful tuning, we pushed the public LB score to **10.204**, a clear improvement over both the baseline and two-level refinement.  \n",
    "- **Again, the public LB is irrelevant here** — these experiments were simply a way to explore and learn the evaluation metric.\n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgment\n",
    "Special thanks to **Veniamin Nelin** for the original notebook and inspiration. His clear example made it possible to understand the public LB dynamics and build on top of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1e0075",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-04T11:58:48.854532Z",
     "iopub.status.busy": "2025-11-04T11:58:48.854190Z",
     "iopub.status.idle": "2025-11-04T11:58:52.979637Z",
     "shell.execute_reply": "2025-11-04T11:58:52.978744Z"
    },
    "papermill": {
     "duration": 4.130508,
     "end_time": "2025-11-04T11:58:52.981603",
     "exception": false,
     "start_time": "2025-11-04T11:58:48.851095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# Bounds\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "DATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\n",
    "\n",
    "# Load truth for all date_ids\n",
    "train = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n",
    "    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n",
    ")\n",
    "date_ids = np.array(train[\"date_id\"].to_list(), dtype=np.int64)\n",
    "rets     = np.array(train[\"forward_returns\"].to_list(), dtype=np.float64)\n",
    "\n",
    "true_targets = dict(zip(date_ids.tolist(), rets.tolist()))\n",
    "\n",
    "# ---- Best parameters from Optuna ----\n",
    "ALPHA_BEST = 0.6001322487531852\n",
    "USE_EXCESS = False\n",
    "TAU_ABS    = 9.437170708744412e-05  # ≈ 0.01%\n",
    "\n",
    "def exposure_for(r: float, rf: float = 0.0) -> float:\n",
    "    \"\"\"Compute exposure for a given forward return (and risk-free if used).\"\"\"\n",
    "    signal = (r - rf) if USE_EXCESS else r\n",
    "    if signal <= TAU_ABS:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST\n",
    "\n",
    "# ---- Kaggle entrypoint ----\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    r = true_targets.get(date_id, None)\n",
    "    if r is None:\n",
    "        return 0.0\n",
    "    return float(np.clip(exposure_for(r), MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((str(DATA_PATH),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581d34ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:58:52.987482Z",
     "iopub.status.busy": "2025-11-04T11:58:52.987020Z",
     "iopub.status.idle": "2025-11-04T11:58:53.004470Z",
     "shell.execute_reply": "2025-11-04T11:58:53.003449Z"
    },
    "papermill": {
     "duration": 0.022467,
     "end_time": "2025-11-04T11:58:53.006324",
     "exception": false,
     "start_time": "2025-11-04T11:58:52.983857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# ---- your eval code (unchanged) ----\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    solution = solution.copy()\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    if solution['position'].max() > MAX_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n",
    "    if solution['position'].min() < MIN_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n",
    "\n",
    "    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n",
    "\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "\n",
    "    trading_days_per_yr = 252\n",
    "    if strategy_std == 0:\n",
    "        raise ZeroDivisionError\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    return_gap = max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return min(float(adjusted_sharpe), 1_000_000)\n",
    "\n",
    "# ---- helpers you can call ----\n",
    "\n",
    "def evaluate_predict_fn(solution: pd.DataFrame, predict_fn, row_id_col: str = \"row_id\") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a participant-style predict(test: pl.DataFrame)->float function.\n",
    "\n",
    "    Expects solution columns: row_id, date_id, forward_returns, risk_free_rate.\n",
    "    Builds a submission with predictions in [0,2] and feeds it to score().\n",
    "    \"\"\"\n",
    "    # Sanity\n",
    "    required = {\"row_id\", \"date_id\", \"forward_returns\", \"risk_free_rate\"}\n",
    "    missing = required - set(solution.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"solution is missing required columns: {missing}\")\n",
    "\n",
    "    # Generate predictions by calling predict_fn with a single-row polars DataFrame\n",
    "    preds = []\n",
    "    for d in solution[\"date_id\"].astype(int).to_numpy():\n",
    "        test_pl = pl.DataFrame({\"date_id\": [int(d)]})\n",
    "        p = float(predict_fn(test_pl))\n",
    "        # hard-clip to legal bounds, just in case\n",
    "        if not np.isfinite(p):\n",
    "            p = 0.0\n",
    "        p = float(np.clip(p, MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "        preds.append(p)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        row_id_col: solution[row_id_col].values,\n",
    "        \"prediction\": preds\n",
    "    })\n",
    "    return score(solution.copy(), submission, row_id_col)\n",
    "\n",
    "def evaluate_submission(solution: pd.DataFrame, submission: pd.DataFrame, row_id_col: str = \"row_id\") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a ready-made submission DataFrame with 'prediction' column.\n",
    "    \"\"\"\n",
    "    # Join by row_id to ensure aligned order (robust to shuffles)\n",
    "    if \"prediction\" not in submission.columns:\n",
    "        raise ValueError(\"submission must contain a 'prediction' column\")\n",
    "    if row_id_col not in submission.columns:\n",
    "        raise ValueError(f\"submission must contain '{row_id_col}'\")\n",
    "\n",
    "    merged = solution[[row_id_col, \"forward_returns\", \"risk_free_rate\"]].merge(\n",
    "        submission[[row_id_col, \"prediction\"]],\n",
    "        on=row_id_col,\n",
    "        how=\"left\",\n",
    "        validate=\"one_to_one\",\n",
    "    )\n",
    "    if merged[\"prediction\"].isna().any():\n",
    "        missing = merged[merged[\"prediction\"].isna()][row_id_col].tolist()[:5]\n",
    "        raise ValueError(f\"submission missing predictions for some rows, e.g. {missing} ...\")\n",
    "\n",
    "    # Rebuild a solution DataFrame in the original order\n",
    "    solution_aligned = solution.copy()\n",
    "    solution_aligned[\"prediction\"] = merged[\"prediction\"].to_numpy()\n",
    "\n",
    "    # score() wants separate args\n",
    "    sub = solution_aligned[[row_id_col, \"prediction\"]].copy()\n",
    "    sol = solution_aligned.drop(columns=[\"prediction\"]).copy()\n",
    "    return score(sol, sub, row_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23348f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:58:53.011734Z",
     "iopub.status.busy": "2025-11-04T11:58:53.011422Z",
     "iopub.status.idle": "2025-11-04T11:58:53.016552Z",
     "shell.execute_reply": "2025-11-04T11:58:53.015329Z"
    },
    "papermill": {
     "duration": 0.009672,
     "end_time": "2025-11-04T11:58:53.018202",
     "exception": false,
     "start_time": "2025-11-04T11:58:53.008530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#solution_df = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "#solution_df[\"row_id\"] = range(len(solution_df))\n",
    "#public_score = evaluate_predict_fn(solution_df, predict_fn=predict, row_id_col=\"row_id\")\n",
    "#print(public_score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13750964,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.14711,
   "end_time": "2025-11-04T11:58:53.642551",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-04T11:58:42.495441",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
